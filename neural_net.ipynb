{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist_get_images import get_images\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = './mnist_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (60000, 28, 28) \n",
      "Testing data dimensions: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train_num, y_train_num, X_test_num, y_test_num = get_images(mnist_path)\n",
    "# Testing data\n",
    "print(f'Training data dimensions: {X_train_num.shape} \\nTesting data dimensions: {X_test_num.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to vectors in float32\n",
    "\n",
    "In this section, the data is prepared and split for training, validation, and testing of a neural network.\n",
    "\n",
    "1. `X_train` and `y_train`: The first 50,000 examples from the original training data (`X_train_num` and `y_train_num`) are selected, reshaped, and normalized to be used for model training.\n",
    "2. `x_validation` and `y_validation`: The last 10,000 examples from the original training data are selected to be used as the validation set.\n",
    "3. `X_test` and `y_test`: The original test data (`X_test_num` and `y_test_num`) are copied and reshaped to be used for the final evaluation of the model.\n",
    "\n",
    "Additionally, the maximum values of `X_train` and the shapes of the datasets `X_train`, `y_train`, `X_test`, and `y_test` are printed to verify that the data preparation has been done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1.0 \n",
      "\n",
      "X_train: (50000, 784)\n",
      "y_train: (50000, 1)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_num[:50_000].reshape(50_000, -1).astype(np.float32)/255\n",
    "y_train = y_train_num[:50_000].reshape(50_000, 1)\n",
    "\n",
    "x_validation = X_train_num[50_000:].reshape(10_000, -1).astype(np.float32)/255\n",
    "y_validation = y_train_num[50_000:].reshape(10_000, 1)\n",
    "\n",
    "X_test = X_test_num.copy().reshape(10_000, -1).astype(np.float32)/255\n",
    "y_test = y_test_num.copy().reshape(10_000, 1)\n",
    "\n",
    "# Testing set\n",
    "print(f'X_train: {X_train.max()} \\n')\n",
    "\n",
    "# Shape\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shown corresponds to: [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAJIUlEQVR4nO3cP6iPfx/H8eurg04i4lCKDBZ/kkkG2ZThZJCBxWBjUigGLCYRMVkUxUIpFoqNDKQzSFJKokRJKDG47uG+f69+9033/X1f9/me4/B4zNer65Ph+zzX4NNr27ZtAKBpmmmTfQAAfh2iAECIAgAhCgCEKAAQogBAiAIAIQoAxFC/D/Z6vUGeA4AB6+f/KvtSACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAghib7APw5er1ep922bdvKmyNHjpQ3c+fOLW8uXbpU3vzq5syZU97s3r27vGnbtryZSGfOnClv9u7dO/4HmWC+FAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCi1/Z5K1XXy8zgLzNmzOi0+/LlyzifBP63Dx8+lDfz588f/4OMo35+7n0pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMTQZB+AP0efdy/+YGxsrLxZu3Ztp3cxcT59+tRp9/Tp0/Lm3Llz5c2DBw/Km9+BLwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAotf2eXVlr9cb9Fngp9avX1/e3Lt3bwAn+dG7d+/Km0ePHnV617Jly8qb5cuXlze3b98ub27cuFHe3L9/v7xpmm635vJP/fzc+1IAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfiMWGmT5/eaffq1avyZsGCBZ3eVXX8+PHy5tChQ53eNTIyUt4sWrSovHn8+HF5w9TgQjwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfi0cnq1avLm64XwW3fvr3Trurhw4flzcaNG8ubr1+/ljcwHlyIB0CJKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAxNNkHYPJNnz69vOlyud1EXWzXNE1z/fr18ubw4cPljcvt+N34UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgem3btn092OsN+iyMg5kzZ5Y3Dx8+LG9WrlxZ3nT15s2b8mbTpk3lzZMnT8obmEr6+bn3pQBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQQ5N9AH5ueHi40+7s2bPlzURebtfFq1evyptdu3aVN+fPny9vunj+/Hmn3devX8f5JPAjXwoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIA0Wvbtu3rwV5v0Gfhb9atW9dpd//+/XE+CeNt586dnXZXrlwpb759+9bpXfye+vm596UAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEEOTfQD401y8eLHTbuHCheXNqVOnOr2LP5cvBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYDotW3b9vVgrzfos/A3w8PDnXYHDx4sb5YsWdLpXVXfv3/vtDtx4kR5s2DBgvLm0KFD5c3mzZvLm67ev39f3mzatKm8GRsbK2+YGvr5ufelAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEC4JRX+ZcaMGeXNyZMny5s9e/aUN10dPXq0vDl27NgATsKvwC2pAJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAuxIP/w5YtW8qba9euDeAkP/fx48fyZt68eQM4Cb8CF+IBUCIKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQAxN9gGmmqVLl5Y379+/L28+f/5c3sB/unPnzmQfgSnGlwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAuBCv6MWLF+XNrVu3ypsdO3aUN03TNB8+fOi0o5vR0dHJPsJ/NTY2NtlHYIrxpQBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQvbZt274e7PUGfZYp4fv37+VNn//E/+bmzZvlTdM0zYkTJ8qbBw8elDefP38ubybSrFmzyputW7eWN6dPny5v5s6dW950tWLFivLm2bNnAzgJv4J+fot8KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQbkkt2rBhQ3lz4cKF8mbZsmXlTVdPnjwpb+7evVve3Llzp7xpmm43nu7bt6+8WbVqVXkzkS5fvlze7Ny5s7zpcqsvU4NbUgEoEQUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgXIg3AUZGRsqbLheZNU3T7N27t7xZvHhxp3fRzdWrVzvtDhw4UN68fPmy07v4PbkQD4ASUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQDChXi/mdmzZ5c3a9asKW9GR0fLm2nTuv0Nsn///k67qtevX5c3ly5dKm9OnTpV3jRN07x9+7bTDv7iQjwASkQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACBfiAfwhXIgHQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAEAM9ftg27aDPAcAvwBfCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMQ/AOc7S+hkG/OqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(len(y_test))\n",
    "print(f'The image shown corresponds to: {y_test[rand_idx]}')\n",
    "plot_img_number(X_test_num[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Forward Propagation and Cost Function in Neural Networks\n",
    "\n",
    "This set of equations describes the flow of a simple neural network model, consisting of an input layer, a hidden layer with ReLU activation function, and an output layer with Softmax activation function for classification. Then, the cross-entropy loss function is used to calculate the error between the predictions and the true values, and finally, the total cost is calculated as the average of the losses across all samples.\n",
    "\n",
    "1. **Forward Propagation (First Hidden Layer):**\n",
    "\n",
    "   $$ z^{(1)} = W^{(1)} X + b^{(1)} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ z^{(1)} $ is the linear output of the first layer.\n",
    "   - $ W^{(1)} $ is the weight matrix of the first layer.\n",
    "   - $ X $ is the input feature set.\n",
    "   - $ b^{(1)} $ is the bias vector of the first layer.\n",
    "\n",
    "2. **Activation (ReLU) in the First Layer:**\n",
    "\n",
    "   $$ a^{(1)} = \\text{ReLU}(z^{(1)}) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ a^{(1)} $ is the output of the ReLU activation function, which applies a non-linear transformation to $ z^{(1)} $.\n",
    "\n",
    "3. **Forward Propagation (Second Output Layer):**\n",
    "\n",
    "   $$ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ z^{(2)} $ is the linear output of the second layer.\n",
    "   - $ W^{(2)} $ is the weight matrix of the second layer.\n",
    "   - $ a^{(1)} $ is the output of the first layer after activation.\n",
    "   - $ b^{(2)} $ is the bias vector of the second layer.\n",
    "\n",
    "4. **Softmax Activation Function (to obtain the probabilities of each class):**\n",
    "\n",
    "   $$ \\hat{y} = \\frac{e^{z_k^{(2)}}}{\\sum_j e^{z_j^{(2)}}} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ \\hat{y} $ represents the predicted probabilities for each class.\n",
    "   - $ e^{z_k^{(2)}} $ is the exponential of the output $ z_k^{(2)} $ for a class $ k $.\n",
    "   - $ \\sum_j e^{z_j^{(2)}} $ is the sum of exponentials for all classes $ j $, ensuring that the probabilities sum to 1.\n",
    "\n",
    "5. **Loss Function (Cross-Entropy):**\n",
    "\n",
    "   $$ \\mathcal{L}(\\hat{y}^i, y^i) = -y^i \\ln(\\hat{y}^i) = -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ \\mathcal{L}(\\hat{y}^i, y^i) $ is the loss for sample $ i $.\n",
    "   - $ y^i $ is the true class value (in one-hot encoding).\n",
    "   - $ \\hat{y}^i $ is the predicted probability for the true class.\n",
    "\n",
    "6. **Average Cost Function for the Entire Dataset:**\n",
    "\n",
    "   $$ J(w, b) = \\frac{1}{\\text{num\\_samples}} \\sum_{i=1}^{\\text{num\\_samples}} -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ J(w, b) $ is the total cost of the model, calculated as the average loss across all samples.\n",
    "   - $ \\text{num\\_samples} $ is the total number of samples in the dataset.\n",
    "   - $ -\\ln(\\hat{y}^i) $ is the loss for sample $ i $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batches "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layers 200-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    \"\"\"\n",
    "    x = (number of samples, 784)\n",
    "    y = (nuumber of samples, 1)\n",
    "    \"\"\"\n",
    "    assert x.shape[0] == y.shape[0], \"Error in the number of samples\"\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x, y = x[idxs], y[idxs]\n",
    "\n",
    "    return (\n",
    "        (x[i : i + mb_size], y[i : i + mb_size]) for i in range(0, total_data, mb_size)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(input_size, neurons):\n",
    "    \"\"\"\n",
    "    input_size = (input elements, 784)\n",
    "    neurons = list[200, 10] amount of neurons in every layer\n",
    "    \"\"\"\n",
    "    w1 = np.random.randn(neurons[0], input_size) * 0.001\n",
    "    b1 = np.zeros((neurons[0], 1))\n",
    "\n",
    "    w2 = np.random.randn(neurons[1], neurons[0]) * 0.001\n",
    "    b2 = np.zeros((neurons[1], 1))\n",
    "\n",
    "    return {\"w1\": w1, \"b1\": b1, \"w2\": w2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 784)\n",
      "(10, 200)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters(28 * 28, [200, 10])\n",
    "print(f'{parameters[\"w1\"].shape}')\n",
    "print(f'{parameters[\"w2\"].shape}')\n",
    "print(f'{parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(x, parameters, act_function):\n",
    "    \"\"\"'\n",
    "    X has shape (number of pixels, number of samples)\n",
    "    \"\"\"\n",
    "    z1 = parameters[\"w1\"] @ x + parameters[\"b1\"]\n",
    "    a1 = act_function(z1)  # Return action function\n",
    "    z2 = parameters[\"w2\"] @ a1 + parameters[\"b2\"]\n",
    "\n",
    "    return z2, z1, a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scores, z1, a1 = scores(X_train[:64].T, parameters, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:64].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    sum_exp_scores = np.sum(exp_scores, axis=0)\n",
    "    probs = exp_scores / sum_exp_scores\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_entropy(scores, y, batch_size=64):\n",
    "    probs = softmax(scores)\n",
    "    y_hat = probs[y. squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "\n",
    "    return probs, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.squeeze()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(probs, x, y, z1, a1, parameters, batch_size=64):\n",
    "    grads = {}\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # y-hat - y\n",
    "    dz2 = probs.copy()\n",
    "\n",
    "    dW2 = dz2 @ a1.T / batch_size\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / batch_size\n",
    "    da1 = parameters[\"w2\"].T @ dz2\n",
    "\n",
    "    dz1 = da1.copy()\n",
    "    dz1[z1 <= 0] = 0\n",
    "\n",
    "    dW1 = dz1 @ x\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    assert parameters[\"w1\"].shape == dW1.shape, \"W1 shape mismatch\"\n",
    "    assert parameters[\"w2\"].shape == dW2.shape, \"W2 shape mismatch\"\n",
    "    assert parameters[\"b1\"].shape == db1.shape, \"b1 shape mismatch\"\n",
    "    assert parameters[\"b2\"].shape == db2.shape, \"b1 shape mismatch\"\n",
    "\n",
    "    grads = {\"w1\": dW1, \"b1\": db1, \"w2\": dW2, \"b2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat, cost = x_entropy(output_scores, y_train[:64])\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward(y_hat, X_train[:64], y_train[:64], z1, a1, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X_data, y_data, mb_size=64):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in create_minibatches(mb_size, X_data, y_data):\n",
    "        points, _, _ = scores(x.T, parameters, relu)\n",
    "        y_hat, _ = x_entropy(points, y, batch_size=len(x))\n",
    "\n",
    "        correct += np.sum(np.argmax(y_hat, axis=0) == y.squeeze())\n",
    "        total += len(y)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, parameters, mb_size=64, learning_rate=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in create_minibatches(mb_size, X_train, y_train):\n",
    "            points, z1, a1 = scores(x.T, parameters=parameters, act_function=relu)\n",
    "            y_hat, cost = x_entropy(points, y, batch_size=len(x))\n",
    "            grads = backward(y_hat, x, y, z1, a1, parameters, batch_size=len(x))\n",
    "\n",
    "            # Update parameters\n",
    "            for param in parameters:\n",
    "                parameters[param] -= learning_rate * grads[param]\n",
    "\n",
    "        # Print cost and accuracy for each epoch\n",
    "        validation_accuracy = accuracy(x_validation, y_validation, mb_size)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Cost: {cost:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Cost: 0.0257, Validation Accuracy: 0.9779\n",
      "Epoch 2/20 - Cost: 0.0263, Validation Accuracy: 0.9783\n",
      "Epoch 3/20 - Cost: 0.0220, Validation Accuracy: 0.9783\n",
      "Epoch 4/20 - Cost: 0.0266, Validation Accuracy: 0.9773\n",
      "Epoch 5/20 - Cost: 0.0106, Validation Accuracy: 0.9778\n",
      "Epoch 6/20 - Cost: 0.0178, Validation Accuracy: 0.9782\n",
      "Epoch 7/20 - Cost: 0.0149, Validation Accuracy: 0.9778\n",
      "Epoch 8/20 - Cost: 0.0220, Validation Accuracy: 0.9779\n",
      "Epoch 9/20 - Cost: 0.0169, Validation Accuracy: 0.9786\n",
      "Epoch 10/20 - Cost: 0.0178, Validation Accuracy: 0.9782\n",
      "Epoch 11/20 - Cost: 0.0112, Validation Accuracy: 0.9779\n",
      "Epoch 12/20 - Cost: 0.0119, Validation Accuracy: 0.9781\n",
      "Epoch 13/20 - Cost: 0.0183, Validation Accuracy: 0.9777\n",
      "Epoch 14/20 - Cost: 0.0210, Validation Accuracy: 0.9775\n",
      "Epoch 15/20 - Cost: 0.0137, Validation Accuracy: 0.9779\n",
      "Epoch 16/20 - Cost: 0.0224, Validation Accuracy: 0.9780\n",
      "Epoch 17/20 - Cost: 0.0240, Validation Accuracy: 0.9777\n",
      "Epoch 18/20 - Cost: 0.0148, Validation Accuracy: 0.9787\n",
      "Epoch 19/20 - Cost: 0.0140, Validation Accuracy: 0.9787\n",
      "Epoch 20/20 - Cost: 0.0153, Validation Accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "mb_size = 512\n",
    "learning_rate = 1e-2\n",
    "epochs = 20\n",
    "parameters = train(epochs, parameters, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
