{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist_get_images import get_images\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = './mnist_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (60000, 28, 28) \n",
      "Testing data dimensions: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train_num, y_train_num, X_test_num, y_test_num = get_images(mnist_path)\n",
    "# Testing data\n",
    "print(f'Training data dimensions: {X_train_num.shape} \\nTesting data dimensions: {X_test_num.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to vectors in float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1.0 \n",
      "\n",
      "X_train: (50000, 784)\n",
      "y_train: (50000, 1)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_num[:50_000].reshape(50_000, -1).astype(np.float32)/255\n",
    "y_train = y_train_num[:50_000].reshape(50_000, 1)\n",
    "\n",
    "x_validation = X_train_num[50_000:].reshape(10_000, -1).astype(np.float32)/255\n",
    "y_validation = y_train_num[50_000:].reshape(10_000, 1)\n",
    "\n",
    "X_test = X_test_num.copy().reshape(10_000, -1).astype(np.float32)/255\n",
    "y_test = y_test_num.copy().reshape(10_000, 1)\n",
    "\n",
    "# Testing set\n",
    "print(f'X_train: {X_train.max()} \\n')\n",
    "\n",
    "# Shape\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shown corresponds to: [8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(len(y_test))\n",
    "print(f'The image shown corresponds to: {y_test[rand_idx]}')\n",
    "plot_img_number(X_test_num[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Forward Propagation y Función de Costo en Redes Neuronales\n",
    "\n",
    "Este conjunto de ecuaciones describe el flujo de un modelo de red neuronal simple, que consta de una capa de entrada, una capa oculta con función de activación ReLU, y una capa de salida con función de activación Softmax para clasificación. Luego, se usa la función de pérdida de entropía cruzada (Cross-Entropy Loss) para calcular el error entre las predicciones y los valores verdaderos, y finalmente, se calcula el costo total como el promedio de las pérdidas en todas las muestras.\n",
    "\n",
    "1. **Forward Propagation (Primera capa oculta):**\n",
    "\n",
    "   $$ z^{(1)} = W^{(1)} X + b^{(1)} $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ z^{(1)} $ es la salida lineal de la primera capa.\n",
    "   - $ W^{(1)} $ es la matriz de pesos de la primera capa.\n",
    "   - $ X $ es el conjunto de características de entrada.\n",
    "   - $ b^{(1)} $ es el vector de sesgo de la primera capa.\n",
    "\n",
    "2. **Activación (ReLU) en la primera capa:**\n",
    "\n",
    "   $$ a^{(1)} = \\text{ReLU}(z^{(1)}) $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ a^{(1)} $ es la salida de la función de activación ReLU, que aplica una transformación no lineal a $ z^{(1)} $.\n",
    "\n",
    "3. **Forward Propagation (Segunda capa de salida):**\n",
    "\n",
    "   $$ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ z^{(2)} $ es la salida lineal de la segunda capa.\n",
    "   - $ W^{(2)} $ es la matriz de pesos de la segunda capa.\n",
    "   - $ a^{(1)} $ es la salida de la primera capa después de la activación.\n",
    "   - $ b^{(2)} $ es el vector de sesgo de la segunda capa.\n",
    "\n",
    "4. **Función de activación Softmax (para obtener las probabilidades de cada clase):**\n",
    "\n",
    "   $$ \\hat{y} = \\frac{e^{z_k^{(2)}}}{\\sum_j e^{z_j^{(2)}}} $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ \\hat{y} $ representa las probabilidades predichas para cada clase.\n",
    "   - $ e^{z_k^{(2)}} $ es la exponencial de la salida $ z_k^{(2)} $ para una clase $ k $.\n",
    "   - $ \\sum_j e^{z_j^{(2)}} $ es la suma de exponenciales para todas las clases $ j $, lo que asegura que las probabilidades sumen 1.\n",
    "\n",
    "5. **Función de pérdida (Cross-Entropy):**\n",
    "\n",
    "   $$ \\mathcal{L}(\\hat{y}^i, y^i) = -y^i \\ln(\\hat{y}^i) = -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ \\mathcal{L}(\\hat{y}^i, y^i) $ es la pérdida para la muestra $ i $.\n",
    "   - $ y^i $ es el valor verdadero de la clase (en codificación one-hot).\n",
    "   - $ \\hat{y}^i $ es la probabilidad predicha para la clase verdadera.\n",
    "\n",
    "6. **Función de costo promedio para todo el conjunto de datos:**\n",
    "\n",
    "   $$ J(w, b) = \\frac{1}{\\text{num\\_samples}} \\sum_{i=1}^{\\text{num\\_samples}} -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Donde:\n",
    "   - $ J(w, b) $ es el costo total del modelo, calculado como el promedio de la pérdida en todas las muestras.\n",
    "   - $ \\text{num\\_samples} $ es el número total de muestras en el conjunto de datos.\n",
    "   - $ -\\ln(\\hat{y}^i) $ es la pérdida para la muestra $ i $.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batches "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layers 200-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    \"\"\"\n",
    "    x = (number of samples, 784)\n",
    "    y = (nuumber of samples, 1)\n",
    "    \"\"\"\n",
    "    assert x.shape[0] == y.shape[0], \"Error in the number of samples\"\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle:\n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x, y = x[idxs], y[idxs]\n",
    "\n",
    "    return (\n",
    "        (x[i : i + mb_size], y[i : i + mb_size]) for i in range(0, total_data, mb_size)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(input_size, neurons):\n",
    "    \"\"\"\n",
    "    input_size = (input elements, 784)\n",
    "    neurons = list[200, 10] amount of neurons in every layer\n",
    "    \"\"\"\n",
    "    w1 = np.random.randn(neurons[0], input_size) * 0.001\n",
    "    b1 = np.zeros((neurons[0], 1))\n",
    "\n",
    "    w2 = np.random.randn(neurons[1], neurons[0]) * 0.001\n",
    "    b2 = np.zeros((neurons[1], 1))\n",
    "\n",
    "    return {\"w1\": w1, \"b1\": b1, \"w2\": w2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 784)\n",
      "(10, 200)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters(28 * 28, [200, 10])\n",
    "print(f'{parameters[\"w1\"].shape}')\n",
    "print(f'{parameters[\"w2\"].shape}')\n",
    "print(f'{parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(x, parameters, act_function):\n",
    "    \"\"\"'\n",
    "    X has shape (number of pixels, number of samples)\n",
    "    \"\"\"\n",
    "    z1 = parameters[\"w1\"] @ x + parameters[\"b1\"]\n",
    "    a1 = act_function(z1)  # Return action function\n",
    "    z2 = parameters[\"w2\"] @ a1 + parameters[\"b2\"]\n",
    "\n",
    "    return z2, z1, a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, z1, a1, = scores(X_train[:64].T, parameters, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:64].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    sum_exp_scores = np.sum(exp_scores, axis=0)\n",
    "    probs = exp_scores / sum_exp_scores\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_entropy(scores, y, batch_size=64):\n",
    "    probs = softmax(scores)\n",
    "    y_hat = probs[y. squeeze(), np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "\n",
    "    return probs, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.squeeze()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(probs, x, y, z1, a1, parameters, batch_size=64):\n",
    "    grads = {}\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # y-hat - y\n",
    "    dz2 = probs.copy()\n",
    "\n",
    "    dW2 = dz2 @ a1.T / batch_size\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / batch_size\n",
    "    da1 = parameters[\"w2\"].T @ dz2\n",
    "\n",
    "    dz1 = da1.copy()\n",
    "    dz1[z1 <= 0] = 0\n",
    "\n",
    "    dW1 = dz1 @ x\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    assert parameters[\"w1\"].shape == dW1.shape, \"W1 shape mismatch\"\n",
    "    assert parameters[\"w2\"].shape == dW2.shape, \"W2 shape mismatch\"\n",
    "    assert parameters[\"b1\"].shape == db1.shape, \"b1 shape mismatch\"\n",
    "    assert parameters[\"b2\"].shape == db2.shape, \"b1 shape mismatch\"\n",
    "\n",
    "    grads = {\"w1\": dW1, \"b1\": db1, \"w2\": dW2, \"b2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat, cost = x_entropy(scores, y_train[:64])\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward(y_hat, X_train[:64], y_train[:64], z1, a1, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X_data, y_data, mb_size=64):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (x, y) in enumerate(create_minibatches(mb_size, X_data, y_data)):\n",
    "        points, z1, a1 = scores(x.T, parameters, relu)\n",
    "        y_hat, cost = x_entropy(points, y, batch_size=len(x))\n",
    "\n",
    "        correct += np.sum(np.argmax(y_hat, axis=0) == y.squeeze())\n",
    "        total += y_hat.shape[1]\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, parameters, mb_size=64, learning_rate=1e-3):\n",
    "    for _ in range(epochs):\n",
    "        for i, (x, y) in enumerate(create_minibatches(mb_size, X_train, y_train)):\n",
    "            points, z1, a1 = scores(x.T, parameters, relu)\n",
    "            y_hat, cost = x_entropy(points, y, batch_size=len(x))\n",
    "            grads = backward(y_hat, x, y, z1, a1, parameters, batch_size=len(x))\n",
    "\n",
    "            parameters[\"w1\"] = parameters[\"w1\"] - learning_rate * grads[\"w1\"]\n",
    "            parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * grads[\"b1\"]\n",
    "            parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * grads[\"b2\"]\n",
    "            parameters[\"w2\"] = parameters[\"w2\"] - learning_rate * grads[\"w2\"]\n",
    "\n",
    "        print(\n",
    "            f\"Cost is: {cost}, y accuracy is: {accuracy(x_validation, y_validation, mb_size)}\"\n",
    "        )\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(relu))\n\u001b[0;32m----> 5\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmb_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, parameters, mb_size, learning_rate)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(create_minibatches(mb_size, X_train, y_train)):\n\u001b[0;32m----> 4\u001b[0m         points, z1, a1 \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         y_hat, cost \u001b[38;5;241m=\u001b[39m x_entropy(points, y, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[1;32m      6\u001b[0m         grads \u001b[38;5;241m=\u001b[39m backward(y_hat, x, y, z1, a1, parameters, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(x))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "mb_size = 512\n",
    "learning_rate = 1e-2\n",
    "epochs = 20\n",
    "print(type(relu))\n",
    "parameters = train(\n",
    "    epochs=epochs, parameters=parameters, mb_size=mb_size, learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
