{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist_get_images import get_images\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "The MNIST dataset (Modified National Institute of Standards and Technology dataset) is a large collection of handwritten digits that is commonly used for training and testing image processing systems and machine learning models. Here are some key points about the MNIST dataset:\n",
    "\n",
    "1. **Content**: It contains 70,000 grayscale images of handwritten digits from 0 to 9. Each image is 28x28 pixels in size.\n",
    "\n",
    "2. **Training and Testing Split**: The dataset is divided into a training set of 60,000 images and a test set of 10,000 images.\n",
    "\n",
    "3. **Usage**: It is widely used for benchmarking machine learning algorithms, particularly in the field of image recognition and classification.\n",
    "\n",
    "4. **Format**: Each image is represented as a 28x28 pixel grid, where each pixel value ranges from 0 (black) to 255 (white).\n",
    "\n",
    "5. **Labels**: Each image is associated with a label indicating the digit (0-9) that the image represents.\n",
    "\n",
    "The MNIST dataset is popular because it is relatively simple and small, yet it provides a challenging task for algorithms to achieve high accuracy. It serves as a standard benchmark for evaluating the performance of various machine learning models and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_path = './mnist_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (60000, 28, 28) \n",
      "Testing data dimensions: (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_train_num, y_train_num, X_test_num, y_test_num = get_images(mnist_path)\n",
    "\n",
    "# Testing data\n",
    "print(f'Training data dimensions: {X_train_num.shape} \\nTesting data dimensions: {X_test_num.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to vectors in float32\n",
    "\n",
    "In this section, the data is prepared and split for training, validation, and testing of a neural network.\n",
    "\n",
    "1. `X_train` and `y_train`: The first 50,000 examples from the original training data (`X_train_num` and `y_train_num`) are selected, reshaped, and normalized to be used for model training.\n",
    "2. `x_validation` and `y_validation`: The last 10,000 examples from the original training data are selected to be used as the validation set.\n",
    "3. `X_test` and `y_test`: The original test data (`X_test_num` and `y_test_num`) are copied and reshaped to be used for the final evaluation of the model.\n",
    "\n",
    "Additionally, the maximum values of `X_train` and the shapes of the datasets `X_train`, `y_train`, `X_test`, and `y_test` are printed to verify that the data preparation has been done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 1.0 \n",
      "\n",
      "X_train: (50000, 784)\n",
      "y_train: (50000, 1)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_num[:50_000].reshape(50_000, -1).astype(np.float32)/255\n",
    "y_train = y_train_num[:50_000].reshape(50_000, 1)\n",
    "\n",
    "x_validation = X_train_num[50_000:].reshape(10_000, -1).astype(np.float32)/255\n",
    "y_validation = y_train_num[50_000:].reshape(10_000, 1)\n",
    "\n",
    "X_test = X_test_num.copy().reshape(10_000, -1).astype(np.float32)/255\n",
    "y_test = y_test_num.copy().reshape(10_000, 1)\n",
    "\n",
    "# Testing set\n",
    "print(f'X_train: {X_train.max()} \\n')\n",
    "\n",
    "# Shape\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph image\n",
    "\n",
    "To visualize the images in the dataset, we can use the `plot_img_number` function defined earlier. Below is an example of how to plot a random image from the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_number(image):\n",
    "    plt.imshow(image.squeeze(), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shown corresponds to: [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_idx = np.random.randint(len(y_test))\n",
    "print(f'The image shown corresponds to: {y_test[rand_idx]}')\n",
    "plot_img_number(X_test_num[rand_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Forward Propagation and Cost Function in Neural Networks\n",
    "\n",
    "This set of equations describes the flow of a simple neural network model, consisting of an input layer, a hidden layer with ReLU activation function, and an output layer with Softmax activation function for classification. Then, the cross-entropy loss function is used to calculate the error between the predictions and the true values, and finally, the total cost is calculated as the average of the losses across all samples.\n",
    "\n",
    "1. **Forward Propagation (First Hidden Layer):**\n",
    "\n",
    "   $$ z^{(1)} = W^{(1)} X + b^{(1)} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ z^{(1)} $ is the linear output of the first layer.\n",
    "   - $ W^{(1)} $ is the weight matrix of the first layer.\n",
    "   - $ X $ is the input feature set.\n",
    "   - $ b^{(1)} $ is the bias vector of the first layer.\n",
    "\n",
    "2. **Activation (ReLU) in the First Layer:**\n",
    "\n",
    "   $$ a^{(1)} = \\text{ReLU}(z^{(1)}) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ a^{(1)} $ is the output of the ReLU activation function, which applies a non-linear transformation to $ z^{(1)} $.\n",
    "\n",
    "3. **Forward Propagation (Second Output Layer):**\n",
    "\n",
    "   $$ z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ z^{(2)} $ is the linear output of the second layer.\n",
    "   - $ W^{(2)} $ is the weight matrix of the second layer.\n",
    "   - $ a^{(1)} $ is the output of the first layer after activation.\n",
    "   - $ b^{(2)} $ is the bias vector of the second layer.\n",
    "\n",
    "4. **Softmax Activation Function (to obtain the probabilities of each class):**\n",
    "\n",
    "   $$ \\hat{y} = \\frac{e^{z_k^{(2)}}}{\\sum_j e^{z_j^{(2)}}} $$\n",
    "   \n",
    "   Where:\n",
    "   - $ \\hat{y} $ represents the predicted probabilities for each class.\n",
    "   - $ e^{z_k^{(2)}} $ is the exponential of the output $ z_k^{(2)} $ for a class $ k $.\n",
    "   - $ \\sum_j e^{z_j^{(2)}} $ is the sum of exponentials for all classes $ j $, ensuring that the probabilities sum to 1.\n",
    "\n",
    "5. **Loss Function (Cross-Entropy):**\n",
    "\n",
    "   $$ \\mathcal{L}(\\hat{y}^i, y^i) = -y^i \\ln(\\hat{y}^i) = -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ \\mathcal{L}(\\hat{y}^i, y^i) $ is the loss for sample $ i $.\n",
    "   - $ y^i $ is the true class value (in one-hot encoding).\n",
    "   - $ \\hat{y}^i $ is the predicted probability for the true class.\n",
    "\n",
    "6. **Average Cost Function for the Entire Dataset:**\n",
    "\n",
    "   $$ J(w, b) = \\frac{1}{\\text{num\\_samples}} \\sum_{i=1}^{\\text{num\\_samples}} -\\ln(\\hat{y}^i) $$\n",
    "   \n",
    "   Where:\n",
    "   - $ J(w, b) $ is the total cost of the model, calculated as the average loss across all samples.\n",
    "   - $ \\text{num\\_samples} $ is the total number of samples in the dataset.\n",
    "   - $ -\\ln(\\hat{y}^i) $ is the loss for sample $ i $."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Creating and Training the Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 layers 200-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create minibatches from input data.\n",
    "\n",
    "    Args:\n",
    "        mb_size (int): The size of each minibatch.\n",
    "        x (ndarray): The input data of shape (number of samples, 784).\n",
    "        y (ndarray): The target data of shape (number of samples, 1).\n",
    "        shuffle (bool, optional): Whether to shuffle the data before creating minibatches. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        generator: A generator that yields minibatches of size mb_size from the input data.\n",
    "    \"\"\"\n",
    "    assert x.shape[0] == y.shape[0], \"Error in the number of samples\"\n",
    "    total_data = x.shape[0]\n",
    "    indices = np.arange(total_data)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, total_data, mb_size):\n",
    "        end_idx = min(start_idx + mb_size, total_data)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield x[batch_indices], y[batch_indices]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init parameters\n",
    "\n",
    "The function uses He initialization for the weights, which helps maintain the variance of activations across network layers, thus improving convergence during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(input_size, neurons):\n",
    "    \"\"\"\n",
    "    Initializes the parameters for a neural network with one hidden layer.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input elements (e.g., 784 for a 28x28 image).\n",
    "        neurons (list): A list containing the number of neurons in each layer,\n",
    "                        e.g., [200, 10] where 200 is the number of neurons in the hidden layer\n",
    "                        and 10 is the number of neurons in the output layer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the initialized weights and biases:\n",
    "            - \"w1\": Weight matrix for the hidden layer, shape (neurons[0], input_size)\n",
    "            - \"b1\": Bias vector for the hidden layer, shape (neurons[0], 1)\n",
    "            - \"w2\": Weight matrix for the output layer, shape (neurons[1], neurons[0])\n",
    "            - \"b2\": Bias vector for the output layer, shape (neurons[1], 1)\n",
    "    \"\"\"\n",
    "    # He initialization for weights\n",
    "    w1 = np.random.randn(neurons[0], input_size) * np.sqrt(2. / input_size)\n",
    "    b1 = np.zeros((neurons[0], 1))\n",
    "\n",
    "    w2 = np.random.randn(neurons[1], neurons[0]) * np.sqrt(2. / neurons[0])\n",
    "    b2 = np.zeros((neurons[1], 1))\n",
    "\n",
    "    return {\"w1\": w1, \"b1\": b1, \"w2\": w2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix for hidden layer (w1) shape: (200, 784)\n",
      "Weight matrix for output layer (w2) shape: (10, 200)\n",
      "Bias vector for output layer (b2) shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters(28 * 28, [200, 10])\n",
    "\n",
    "print(f'Weight matrix for hidden layer (w1) shape: {parameters[\"w1\"].shape}')\n",
    "print(f'Weight matrix for output layer (w2) shape: {parameters[\"w2\"].shape}')\n",
    "print(f'Bias vector for output layer (b2) shape: {parameters[\"b2\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(x, parameters, act_function):\n",
    "    \"\"\"\n",
    "    Computes the scores for the neural network with one hidden layer.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Input data of shape (number of pixels, number of samples).\n",
    "        parameters (dict): Dictionary containing the weights and biases:\n",
    "            - \"w1\": Weight matrix for the hidden layer, shape (neurons[0], input_size)\n",
    "            - \"b1\": Bias vector for the hidden layer, shape (neurons[0], 1)\n",
    "            - \"w2\": Weight matrix for the output layer, shape (neurons[1], neurons[0])\n",
    "            - \"b2\": Bias vector for the output layer, shape (neurons[1], 1)\n",
    "        act_function (function): Activation function to be applied in the hidden layer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - z2 (ndarray): Output scores of the network, shape (neurons[1], number of samples)\n",
    "            - z1 (ndarray): Linear output of the hidden layer, shape (neurons[0], number of samples)\n",
    "            - a1 (ndarray): Activated output of the hidden layer, shape (neurons[0], number of samples)\n",
    "    \"\"\"\n",
    "    z1 = np.dot(parameters[\"w1\"], x) + parameters[\"b1\"]\n",
    "    a1 = act_function(z1)\n",
    "    z2 = np.dot(parameters[\"w2\"], a1) + parameters[\"b2\"]\n",
    "\n",
    "    return z2, z1, a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_scores, z1, a1 = scores(X_train[:64].T, parameters, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:64].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters:\n",
    "    x (ndarray): Input data, typically a 2D array where each column represents a different data point.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: The softmax probabilities of the input data, with the same shape as the input.\n",
    "    \"\"\"\n",
    "    exp_scores = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=0, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x_entropy(scores, y, batch_size=64):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss and the probabilities for a given set of scores and true labels.\n",
    "\n",
    "    Args:\n",
    "        scores (np.ndarray): The predicted scores (logits) for each class, shape (num_classes, batch_size).\n",
    "        y (np.ndarray): The true labels, shape (batch_size, 1).\n",
    "        batch_size (int, optional): The number of samples in the batch. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - probs (np.ndarray): The probabilities for each class, shape (num_classes, batch_size).\n",
    "            - cost (float): The cross-entropy loss.\n",
    "    \"\"\"\n",
    "    probs = softmax(scores)\n",
    "    y_hat = probs[y.squeeze(), np.arange(batch_size)]\n",
    "    cost = -np.mean(np.log(y_hat))\n",
    "    return probs, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.squeeze()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(probs, x, y, z1, a1, parameters, batch_size=64):\n",
    "    \"\"\"\n",
    "    Perform the backward pass for a two-layer neural network.\n",
    "\n",
    "    Args:\n",
    "        probs (numpy.ndarray): The probabilities output from the forward pass (shape: [output_size, batch_size]).\n",
    "        x (numpy.ndarray): The input data (shape: [input_size, batch_size]).\n",
    "        y (numpy.ndarray): The true labels (shape: [1, batch_size]).\n",
    "        z1 (numpy.ndarray): The linear output from the first layer (shape: [hidden_size, batch_size]).\n",
    "        a1 (numpy.ndarray): The activation output from the first layer (shape: [hidden_size, batch_size]).\n",
    "        parameters (dict): Dictionary containing the network parameters:\n",
    "            - \"w1\" (numpy.ndarray): Weights of the first layer (shape: [hidden_size, input_size]).\n",
    "            - \"b1\" (numpy.ndarray): Biases of the first layer (shape: [hidden_size, 1]).\n",
    "            - \"w2\" (numpy.ndarray): Weights of the second layer (shape: [output_size, hidden_size]).\n",
    "            - \"b2\" (numpy.ndarray): Biases of the second layer (shape: [output_size, 1]).\n",
    "        batch_size (int, optional): The size of the batch. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the gradients with respect to the parameters:\n",
    "            - \"w1\" (numpy.ndarray): Gradient of the loss with respect to the weights of the first layer.\n",
    "            - \"b1\" (numpy.ndarray): Gradient of the loss with respect to the biases of the first layer.\n",
    "            - \"w2\" (numpy.ndarray): Gradient of the loss with respect to the weights of the second layer.\n",
    "            - \"b2\" (numpy.ndarray): Gradient of the loss with respect to the biases of the second layer.\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    probs[y.squeeze(), np.arange(batch_size)] -= 1  # y-hat - y\n",
    "    dz2 = probs.copy()\n",
    "\n",
    "    dW2 = dz2 @ a1.T / batch_size\n",
    "    db2 = np.sum(dz2, axis=1, keepdims=True) / batch_size\n",
    "    da1 = parameters[\"w2\"].T @ dz2\n",
    "\n",
    "    dz1 = da1.copy()\n",
    "    dz1[z1 <= 0] = 0\n",
    "\n",
    "    dW1 = dz1 @ x\n",
    "    db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "    assert parameters[\"w1\"].shape == dW1.shape, \"W1 shape mismatch\"\n",
    "    assert parameters[\"w2\"].shape == dW2.shape, \"W2 shape mismatch\"\n",
    "    assert parameters[\"b1\"].shape == db1.shape, \"b1 shape mismatch\"\n",
    "    assert parameters[\"b2\"].shape == db2.shape, \"b1 shape mismatch\"\n",
    "\n",
    "    grads = {\"w1\": dW1, \"b1\": db1, \"w2\": dW2, \"b2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat, cost = x_entropy(output_scores, y_train[:64])\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backward(y_hat, X_train[:64], y_train[:64], z1, a1, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X_data, y_data, mb_size=64):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the model predictions.\n",
    "\n",
    "    Parameters:\n",
    "    X_data (numpy.ndarray): The input data.\n",
    "    y_data (numpy.ndarray): The true labels corresponding to the input data.\n",
    "    mb_size (int, optional): The size of the minibatches. Default is 64.\n",
    "\n",
    "    Returns:\n",
    "    float: The accuracy of the model as a fraction of correct predictions over the total number of predictions.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in create_minibatches(mb_size, X_data, y_data):\n",
    "        points, _, _ = scores(x.T, parameters, relu)\n",
    "        y_hat = np.argmax(points, axis=0)\n",
    "        correct += np.sum(y_hat == y.squeeze())\n",
    "        total += len(y)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, parameters, mb_size=64, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Trains a neural network using mini-batch gradient descent.\n",
    "\n",
    "    Args:\n",
    "        epochs (int): Number of epochs to train the model.\n",
    "        parameters (dict): Dictionary containing the parameters of the neural network.\n",
    "        mb_size (int, optional): Size of the mini-batches. Defaults to 64.\n",
    "        learning_rate (float, optional): Learning rate for the gradient descent. Defaults to 1e-3.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated parameters after training.\n",
    "\n",
    "    The function performs the following steps:\n",
    "    1. Iterates over the specified number of epochs.\n",
    "    2. For each epoch, creates mini-batches from the training data.\n",
    "    3. Computes the scores, cost, and gradients for each mini-batch.\n",
    "    4. Updates the parameters using the computed gradients.\n",
    "    5. Prints the cost and validation accuracy for each epoch.\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in create_minibatches(mb_size, X_train, y_train):\n",
    "            points, z1, a1 = scores(x.T, parameters=parameters, act_function=relu)\n",
    "            y_hat, cost = x_entropy(points, y, batch_size=len(x))\n",
    "            grads = backward(y_hat, x, y, z1, a1, parameters, batch_size=len(x))\n",
    "\n",
    "            # Update parameters\n",
    "            for param in parameters:\n",
    "                parameters[param] -= learning_rate * grads[param]\n",
    "\n",
    "        # Print cost and accuracy for each epoch\n",
    "        validation_accuracy = accuracy(x_validation, y_validation, mb_size)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Cost: {cost:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Cost: 0.3455, Validation Accuracy: 0.9279\n",
      "Epoch 2/20 - Cost: 0.2595, Validation Accuracy: 0.9394\n",
      "Epoch 3/20 - Cost: 0.1929, Validation Accuracy: 0.9505\n",
      "Epoch 4/20 - Cost: 0.1581, Validation Accuracy: 0.9554\n",
      "Epoch 5/20 - Cost: 0.1622, Validation Accuracy: 0.9593\n",
      "Epoch 6/20 - Cost: 0.1372, Validation Accuracy: 0.9616\n",
      "Epoch 7/20 - Cost: 0.1179, Validation Accuracy: 0.9630\n",
      "Epoch 8/20 - Cost: 0.0978, Validation Accuracy: 0.9640\n",
      "Epoch 9/20 - Cost: 0.0993, Validation Accuracy: 0.9658\n",
      "Epoch 10/20 - Cost: 0.1692, Validation Accuracy: 0.9654\n",
      "Epoch 11/20 - Cost: 0.0853, Validation Accuracy: 0.9666\n",
      "Epoch 12/20 - Cost: 0.1074, Validation Accuracy: 0.9663\n",
      "Epoch 13/20 - Cost: 0.0638, Validation Accuracy: 0.9689\n",
      "Epoch 14/20 - Cost: 0.1032, Validation Accuracy: 0.9689\n",
      "Epoch 15/20 - Cost: 0.0837, Validation Accuracy: 0.9701\n",
      "Epoch 16/20 - Cost: 0.1104, Validation Accuracy: 0.9702\n",
      "Epoch 17/20 - Cost: 0.0607, Validation Accuracy: 0.9698\n",
      "Epoch 18/20 - Cost: 0.0731, Validation Accuracy: 0.9701\n",
      "Epoch 19/20 - Cost: 0.0892, Validation Accuracy: 0.9703\n",
      "Epoch 20/20 - Cost: 0.0613, Validation Accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "mb_size = 512\n",
    "learning_rate = 1e-2\n",
    "epochs = 20\n",
    "parameters = train(epochs, parameters, mb_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 98.11%\n",
      "Accuracy on test set: 96.82%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy on train set: {accuracy(X_train, y_train, mb_size):.2%}')\n",
    "print(f'Accuracy on test set: {accuracy(X_test, y_test, mb_size):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x):\n",
    "    \"\"\"\n",
    "    Predicts the class of each input sample.\n",
    "\n",
    "    Args:\n",
    "        x (ndarray): Input data of shape (number of pixels, number of samples).\n",
    "    Returns:\n",
    "        ndarray: Predicted class for each input sample.\n",
    "    \"\"\"\n",
    "    points, _, _ = scores(x.T, parameters, relu)\n",
    "    return np.argmax(points, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
